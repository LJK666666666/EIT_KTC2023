# TPU v5e-8 å†…å­˜ä¼˜åŒ–æŒ‡å—

## é—®é¢˜

```
ValueError: Allocation would exceed memory (size=17179869184)
```

**å«ä¹‰**: å°è¯•åˆ†é…è¶…è¿‡ TPU å¯ç”¨å†…å­˜ï¼ˆ16GBï¼‰çš„å†…å­˜ç©ºé—´ã€‚

---

## âœ… è§£å†³æ–¹æ¡ˆ

### 1. ç«‹å³ä¿®å¤ - ä½¿ç”¨æ›´å°çš„æ‰¹å¤§å°

```bash
# æ–¹æ¡ˆ Aï¼šæ‰¹å¤§å° 16ï¼ˆæ¨èå¼€å§‹ï¼‰
python main.py --mode train \
  --data-path /kaggle/input/simdata-cdeit \
  --global-batch-size 16

# æ–¹æ¡ˆ Bï¼šæ‰¹å¤§å° 8ï¼ˆæ›´ä¿é™©ï¼‰
python main.py --mode train \
  --data-path /kaggle/input/simdata-cdeit \
  --global-batch-size 8

# æ–¹æ¡ˆ Cï¼šæ‰¹å¤§å° 4ï¼ˆå¦‚æœä»ç„¶å†…å­˜ä¸è¶³ï¼‰
python main.py --mode train \
  --data-path /kaggle/input/simdata-cdeit \
  --global-batch-size 4
```

### 2. ä»£ç ä¿®æ”¹ï¼ˆå·²åšï¼‰

éªŒè¯é›†æ‰¹å¤§å°å·²ä¿®æ”¹ï¼š
```python
# åŸæ¥ï¼ˆå†…å­˜å¤šï¼‰
batch_size=batch_size * 4

# ç°åœ¨ï¼ˆä¸º TPU ä¼˜åŒ–ï¼‰
batch_size=batch_size
```

---

## ğŸ“Š TPU v5e-8 å†…å­˜è§„æ ¼

| é¡¹ç›® | è§„æ ¼ |
|------|------|
| æ€»å†…å­˜ | 16 GB |
| å¯ç”¨å†…å­˜ | ~14-15 GBï¼ˆç³»ç»Ÿå ç”¨ï¼‰ |
| å•ç²¾åº¦ï¼ˆFP32ï¼‰ | ~4 äº¿å‚æ•° |
| æ··åˆç²¾åº¦ï¼ˆBF16ï¼‰ | ~8 äº¿å‚æ•° |

---

## ğŸ¯ æ¨èé…ç½®

### ä¿å®ˆé…ç½®ï¼ˆæœ€å®‰å…¨ï¼‰
```bash
python main.py --mode train \
  --global-batch-size 8 \
  --epochs 100 \
  --num-workers 2 \
  --log-every 50
```

### å¹³è¡¡é…ç½®ï¼ˆæ¨èï¼‰
```bash
python main.py --mode train \
  --global-batch-size 16 \
  --epochs 100 \
  --num-workers 4 \
  --log-every 50
```

### æ¿€è¿›é…ç½®ï¼ˆéœ€è¦è¶³å¤Ÿæ•°æ®ï¼‰
```bash
python main.py --mode train \
  --global-batch-size 32 \
  --epochs 50 \
  --num-workers 4
```

---

## å†…å­˜ä¼˜åŒ–æŠ€å·§

### 1. å‡å°æ‰¹å¤§å°ï¼ˆæœ€æœ‰æ•ˆï¼‰

| æ‰¹å¤§å° | å†…å­˜å ç”¨ | é€Ÿåº¦ | æ”¶æ•› |
|--------|---------|------|------|
| 4 | å¾ˆä½ | æ…¢ | ç¨³å®š |
| 8 | ä½ | ä¸­ | è‰¯å¥½ |
| 16 | ä¸­ | å¿« | å¾ˆå¥½ |
| 32 | é«˜ | å¾ˆå¿« | éœ€è¦è°ƒ LR |
| 64 | å¾ˆé«˜ âŒ | è¶…å¿« | å®¹æ˜“ OOM |

### 2. å‡å°éªŒè¯é›†æ‰¹å¤§å°

å·²ä¿®æ”¹ä¸º `batch_size`ï¼ˆåŸä¸º `batch_size * 4`ï¼‰

### 3. å‡å°‘æ•°æ®åŠ è½½è¿›ç¨‹

```bash
python main.py --num-workers 2  # æ”¹ä» 4 ä¸º 2
```

### 4. å¯ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆé«˜çº§ï¼‰

å¦‚æœéœ€è¦å¤§æœ‰æ•ˆæ‰¹å¤§å°ä½†å†…å­˜æœ‰é™ï¼Œå¯ä»¥ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼š

```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­
accumulated_steps = 4
if train_steps % accumulated_steps == 0:
    opt.step()
    opt.zero_grad()
else:
    # æ¢¯åº¦ç¼©æ”¾
    (loss / accumulated_steps).backward()
```

---

## è¯Šæ–­æ­¥éª¤

### 1. æ£€æŸ¥ TPU å†…å­˜ä½¿ç”¨

```python
import torch_xla.core.xla_model as xm

# åœ¨è®­ç»ƒå¾ªç¯ä¸­æ‰“å°å†…å­˜
print(f"TPU å†…å­˜: {xm.get_metrics()}")
```

### 2. ä»å°åˆ°å¤§å°è¯•æ‰¹å¤§å°

```bash
# å…ˆè¯• 4
python main.py --global-batch-size 4 --epochs 1

# å†è¯• 8
python main.py --global-batch-size 8 --epochs 1

# å†è¯• 16
python main.py --global-batch-size 16 --epochs 1

# æ‰¾åˆ°æœ€å¤§å¯ç”¨æ‰¹å¤§å°
```

### 3. ç›‘æ§é”™è¯¯ä¿¡æ¯

å¦‚æœå‡ºç°ç±»ä¼¼é”™è¯¯ï¼Œè®°ä¸‹ allocation sizeï¼š
- **< 4GB**: æ‰¹å¤§å°å¯ä»¥æ›´å¤§
- **4-8GB**: æ‰¹å¤§å°åˆé€‚
- **8-16GB**: æ¥è¿‘æé™
- **> 16GB**: å†…å­˜æº¢å‡º

---

## å¿«é€Ÿä¿®å¤æ£€æŸ¥æ¸…å•

- [ ] ä¿®æ”¹ `--global-batch-size` ä¸º 16 æˆ–æ›´å°
- [ ] ä¿®æ”¹ `loaderVal batch_size` ä» `batch_size * 4` ä¸º `batch_size` âœ… å·²åš
- [ ] ä¿®æ”¹ `--num-workers` ä¸º 2-4
- [ ] ç¡®ä¿ `--epochs` åˆç†ï¼ˆä¸è¦å¤ªå¤§ï¼‰

---

## å¦‚æœè¿˜æ˜¯å†…å­˜ä¸è¶³

### æ£€æŸ¥æ¨¡å‹å¤§å°

```python
import torch
model = DiT()
total_params = sum(p.numel() for p in model.parameters())
print(f"æ¨¡å‹å‚æ•°: {total_params / 1e6:.1f}M")  # å•ä½ç™¾ä¸‡
```

### å¯èƒ½çš„åŸå› 

1. **æ•°æ®å˜é‡æ²¡æœ‰æ¸…ç†** - æ£€æŸ¥è®­ç»ƒå¾ªç¯æ˜¯å¦æœ‰å†…å­˜æ³„æ¼
2. **è¾“å…¥å°ºå¯¸å¤ªå¤§** - æ£€æŸ¥è¾“å…¥ shape æ˜¯å¦æ­£ç¡®
3. **ç´¯ç§¯æ¢¯åº¦** - ç¡®ä¿æ¢¯åº¦åœ¨åå‘ä¼ æ’­åæ¸…é™¤

### æç«¯è§£å†³æ–¹æ¡ˆ

```bash
# å¦‚æœæ¨¡å‹çœŸçš„å¤ªå¤§ï¼Œåªèƒ½ç”¨ 1 çš„æ‰¹å¤§å°
python main.py --global-batch-size 1 --epochs 1
```

---

## æ€§èƒ½é¢„æœŸ

ä½¿ç”¨ä¸åŒæ‰¹å¤§å°çš„è®­ç»ƒé€Ÿåº¦ï¼š

```
æ‰¹å¤§å° 8ï¼š  ~10-15 ç§’/step
æ‰¹å¤§å° 16ï¼š ~12-18 ç§’/step ï¼ˆæ¨èï¼‰
æ‰¹å¤§å° 32ï¼š å†…å­˜ä¸è¶³æˆ–å¾ˆæ…¢
æ‰¹å¤§å° 64ï¼š âŒ å†…å­˜æº¢å‡º
```

---

## éªŒè¯ä¿®å¤æˆåŠŸ

```bash
# 1. å…ˆè·‘ 1 ä¸ª epoch æµ‹è¯•
python main.py --mode train \
  --global-batch-size 16 \
  --epochs 1 \
  --log-every 5

# 2. å¦‚æœæˆåŠŸï¼Œè·‘å®Œæ•´è®­ç»ƒ
python main.py --mode train \
  --global-batch-size 16 \
  --epochs 100
```

---

## æ€»ç»“

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|---------|
| å†…å­˜æº¢å‡º | å‡å° `--global-batch-size` |
| éªŒè¯å¤ªæ…¢ | âœ… å·²å‡å°éªŒè¯é›†æ‰¹å¤§å° |
| ä»ç„¶å†…å­˜ä¸è¶³ | è¿›ä¸€æ­¥å‡å°æ‰¹å¤§å°æˆ– `--num-workers` |
| è®­ç»ƒå¤ªæ…¢ | å¢å¤§æ‰¹å¤§å°ï¼ˆå¦‚æœå†…å­˜å…è®¸ï¼‰ |

**ç«‹å³å°è¯•**ï¼š
```bash
python main.py --mode train --data-path /kaggle/input/simdata-cdeit --global-batch-size 16
```

è¿™åº”è¯¥èƒ½è§£å†³é—®é¢˜ï¼
